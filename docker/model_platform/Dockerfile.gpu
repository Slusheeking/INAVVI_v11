FROM nvcr.io/nvidia/tensorflow:24.02-tf2-py3
LABEL maintainer="ATS Platform Team" 
LABEL description="GPU-optimized ML platform for NVIDIA GH200 Grace Hopper Superchips"

# Platform-specific environment variables for GH200 Grace Hopper
ENV PYTHONUNBUFFERED=1 \
    TZ=UTC \
    PLATFORM=linux/arm64 \
    COMPONENT=model_platform \
    CUDA_MODULE_LOADING=LAZY \
    NVIDIA_VISIBLE_DEVICES=all \
    CUDA_VISIBLE_DEVICES=all \
    USE_GPU=true \
    TF_ENABLE_ONEDNN_OPTS=0 \
    MPLCONFIGDIR=/tmp/matplotlib

# TensorFlow and TensorRT optimization settings for GH200
ENV TF_FORCE_GPU_ALLOW_GROWTH=true \
    XLA_FLAGS="--xla_gpu_cuda_data_dir=/usr/local/cuda" \
    TF_GPU_ALLOCATOR=cuda_malloc_async \
    TF_ENABLE_ONEDNN_OPTS=1 \
    TF_XLA_FLAGS="--tf_xla_auto_jit=2" \
    TF_CUDA_COMPUTE_CAPABILITIES="8.9" \
    TF_EXTRA_CUDA_COMPUTE_CAPABILITIES="9.0" \
    TF_USE_CUDNN=1 \
    TF_CUDNN_DETERMINISTIC=0 \
    TF_CUDNN_USE_AUTOTUNE=1 \
    TF_CUDNN_RESET_RNN_DESCRIPTOR=1 \
    TF_DISABLE_CUDNN_RNN=0

# TensorRT optimization settings
ENV TRT_MAX_WORKSPACE_SIZE=8589934592 \
    TRT_DLA_ENABLE=0 \
    TRT_BUILDER_OPTIMIZATION_LEVEL=3

# GH200 Grace Hopper specific optimizations
ENV NCCL_P2P_LEVEL=NVL \
    NCCL_IB_HCA=mlx5 \
    NCCL_NVLS_ENABLE=1 \
    NCCL_NVLS_MEMOPS_ENABLE=1 \
    NCCL_NVLS_MEMOPS_P2P_ENABLE=1 \
    RAPIDS_ALLOW_NONUNIFORM_MEMORY=1

# Performance settings
ENV OMP_NUM_THREADS=8 \
    KMP_BLOCKTIME=0 \
    TRAINING_BATCH_SIZE=4096 \
    INFERENCE_BATCH_SIZE=8192

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    postgresql-client \
    curl \
    git \
    wget \
    build-essential \
    ca-certificates \
    libc6-dev \
    linux-libc-dev \
    libstdc++-11-dev \
    netcat-openbsd \
    redis-tools \
    gnupg \
    supervisor \
    && rm -rf /var/lib/apt/lists/*

# Create directories
RUN mkdir -p /app/models /app/logs /app/data /app/features /app/cache

# Install TensorFlow addons
RUN pip install --no-cache-dir tensorflow-addons tensorflow-probability tensorflow-hub tensorflow-text tensorflow-io gpustat
# Explicitly install GPU-enabled TensorFlow compatible with CUDA 12.4
# Note: tensorflow-cuda is not a valid package, using base TensorFlow with proper CUDA config
RUN pip install --no-cache-dir tensorflow==2.18.1
# Configure PyTorch settings for GPU
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:4096 \
    TORCH_CUDA_ARCH_LIST="8.9;9.0" \
    TORCH_NVCC_FLAGS="-Xfatbin -compress-all"

# Install PyTorch with CUDA support
RUN pip uninstall -y torch torchvision torchaudio && \
    pip install --no-cache-dir torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0

# Ensure CUDA libraries are properly configured
RUN ldconfig /usr/local/cuda/lib64

# Install GPU testing utilities
RUN pip install --no-cache-dir gpustat

# Create workdir first
RUN mkdir -p /app
WORKDIR /app

# Verify GPU configuration
RUN python -c "import tensorflow as tf; print('TensorFlow version:', tf.__version__); \
    print('GPUs:', tf.config.list_physical_devices('GPU')); \
    print('Built with CUDA:', tf.test.is_built_with_cuda()); \
    print('Built with GPU support:', tf.test.is_built_with_gpu_support())" || true && \
    python -c "import torch; print('PyTorch version:', torch.__version__); \
    print('CUDA available:', torch.cuda.is_available()); \
    print('CUDA device count:', torch.cuda.device_count()); \
    print('CUDA device name:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')" || true && \
    python -c "import tensorflow as tf; print('TF Version:', tf.__version__); \
    print('GPUs Available:', tf.config.list_physical_devices('GPU')); \
    print('Built with CUDA:', tf.test.is_built_with_cuda()); \
    print('GPU support:', tf.test.is_built_with_gpu_support())" || true

ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64

# Add our platform code to the image
COPY . /app/

# Use supervisord to manage processes
COPY docker/model_platform/supervisord.conf /etc/supervisor/conf.d/
COPY docker/model_platform/start.sh /app/start.sh
RUN chmod +x /app/start.sh

# Set PYTHONPATH for module discovery
ENV PYTHONPATH=/app \
    MPLCONFIGDIR=/tmp/matplotlib

# Expose necessary ports
EXPOSE 8000 8001 8002
# Install additional Python dependencies
RUN pip install --no-cache-dir \
    psycopg2-binary \
    redis \
    pandas \
    numpy \
    scikit-learn \
    matplotlib \
    seaborn \
    plotly \
    grpcio \
    xgboost \
    lightgbm \
    optuna \
    mlflow \
    tensorboard \
    tensorflow-serving-api \
    tensorflow-probability \
    tensorflow-addons \
    tensorflow-model-optimization \
    tensorflow-hub \
    tensorflow-text \
    transformers \
    hmmlearn \
    statsmodels \
    scipy \
    pyarrow \
    fastparquet \
    fastapi \
    uvicorn \
    pydantic \
    prometheus-client \
    pyyaml \
    requests \
    python-dotenv \
    pytest \
    pytest-cov \
    flake8 \
    black \
    isort

# Create app directories
WORKDIR /app

# Create necessary directories
RUN mkdir -p /app/logs /app/models/registry /app/models/checkpoints /app/data/backtest /app/results/backtest

# Create non-root user
RUN groupadd -g 1000 appuser && \
    useradd -r -u 1000 -g appuser appuser && \
    chown -R appuser:appuser /app

# Copy application code
COPY --chown=appuser:appuser src/utils /app/src/utils
COPY --chown=appuser:appuser src/model_training /app/src/model_training
COPY --chown=appuser:appuser src/model_services /app/src/model_services
COPY --chown=appuser:appuser src/continuous_learning /app/src/continuous_learning

# Copy TensorFlow Serving REST API implementation
COPY docker/model_platform/tf_serving_rest_api.py /app/docker/model_platform/tf_serving_rest_api.py

# Copy TensorFlow Serving API implementation
COPY docker/model_platform/tf_serving_api.py /app/docker/model_platform/tf_serving_api.py

# Copy fixed TensorFlow Serving API implementation
COPY docker/model_platform/tf_serving_api_fixed.py /app/docker/model_platform/tf_serving_api_fixed.py

# Copy matplotlib fix script
COPY docker/model_platform/fix_matplotlib_permissions.py /app/docker/model_platform/fix_matplotlib_permissions.py

# Copy model registry initialization script
COPY docker/model_platform/init_model_registry.py /app/docker/model_platform/init_model_registry.py

# Copy process monitor script
COPY docker/model_platform/process_monitor.py /app/docker/model_platform/process_monitor.py

# Copy common scripts
COPY docker/common /app/docker/common

# Create start script
COPY --chown=appuser:appuser docker/model_platform/start.sh /app/start.sh

# Set executable permissions
RUN chmod +x /app/docker/common/*.sh
RUN chmod +x /app/start.sh
RUN chmod +x /app/docker/model_platform/fix_matplotlib_permissions.py
RUN chmod +x /app/docker/model_platform/init_model_registry.py
RUN chmod +x /app/docker/model_platform/process_monitor.py

# Setup supervisord
COPY docker/model_platform/supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# Copy TensorFlow serving supervisord config
COPY docker/model_platform/tf_serving_supervisord.conf /etc/supervisor/conf.d/tf_serving_supervisord.conf

# Add healthcheck script
RUN echo '#!/bin/bash\n\
    if curl -s http://localhost:8003/health | grep -q "healthy" && \
    curl -s http://localhost:8005/health | grep -q "healthy"; then\n\
    exit 0\n\
    else\n\
    exit 1\n\
    fi' > /app/healthcheck.sh && \
    chmod +x /app/healthcheck.sh

# Switch to non-root user
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD /app/healthcheck.sh

# Expose ports
EXPOSE 8003 8005 8500 8501

# Start supervisord
CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"]
